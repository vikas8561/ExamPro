# üîß Production Issues & Fixes

## Issues Identified from Logs

### 1. **Running Development Server Instead of Gunicorn** ‚ö†Ô∏è
**Problem:** Logs show `==> Running 'python app.py'` and Flask development server warning

**Cause:** Render dashboard has explicit "Start Command" set to `python app.py` instead of using Procfile

**Fix:** 
1. Go to Render Dashboard ‚Üí Your Face Recognition Service ‚Üí Settings
2. **Remove or clear the "Start Command" field** (leave it empty)
3. Render will automatically use the `Procfile` which has: `web: gunicorn --bind 0.0.0.0:$PORT app:app`
4. Redeploy the service

### 2. **Model Downloading on Every Request** ‚ö†Ô∏è
**Problem:** `vgg_face_weights.h5` (580MB) downloads on every request

**Causes:**
- Service restarts frequently (cold starts on free tier)
- Model cache directory (`~/.deepface/weights/`) gets cleared on restart
- Memory constraints causing service restarts

**Fixes Applied:**
- ‚úÖ Pre-load model at startup (in `app.py`)
- ‚úÖ Suppress TensorFlow warnings to reduce log noise
- ‚úÖ Better error handling

**Additional Recommendations:**
- **Upgrade to Standard plan ($25/month)** - Service stays running, model stays cached
- **Use persistent disk** (if available on your Render plan) to cache model
- **Implement keep-alive** to prevent service from sleeping

### 3. **Memory Warnings** ‚ö†Ô∏è
**Problem:** `Allocation of 411041792 exceeds 10% of free system memory` (411MB on 512MB server)

**Cause:** VGG-Face model is ~400MB, and TensorFlow needs additional memory for processing

**Solutions:**
- ‚úÖ **Upgrade to Standard plan (2GB RAM)** - Recommended minimum
- ‚úÖ Model pre-loading helps (loads once at startup)
- ‚úÖ Suppressed TensorFlow verbose logging

**Memory Breakdown:**
- VGG-Face model: ~400MB
- TensorFlow runtime: ~100-200MB
- Application: ~50MB
- **Total needed: ~550-650MB minimum**
- **512MB server: Too tight, will cause issues**
- **2GB server: Comfortable, recommended**

### 4. **CUDA Errors (Harmless but Noisy)** ‚úÖ Fixed
**Problem:** `failed call to cuInit: INTERNAL: CUDA error`

**Cause:** TensorFlow trying to use GPU, but Render servers don't have GPUs

**Fix Applied:**
- ‚úÖ Set `CUDA_VISIBLE_DEVICES=''` to force CPU mode
- ‚úÖ Set `TF_CPP_MIN_LOG_LEVEL=2` to suppress warnings
- ‚úÖ These errors are now suppressed

### 5. **TensorFlow Warnings** ‚úÖ Fixed
**Problem:** Verbose TensorFlow logging cluttering logs

**Fix Applied:**
- ‚úÖ Set `TF_CPP_MIN_LOG_LEVEL=2` environment variable
- ‚úÖ Suppresses INFO and WARNING messages
- ‚úÖ Only shows ERROR messages

---

## üöÄ Deployment Checklist

### **Step 1: Update Render Configuration**

1. **Go to Render Dashboard ‚Üí Your Face Recognition Service ‚Üí Settings**

2. **Check/Update:**
   - **Start Command:** Leave EMPTY (so it uses Procfile)
   - **Build Command:** `pip install -r requirements.txt`
   - **Plan:** **Standard ($25/month)** recommended (2GB RAM)

3. **Environment Variables (Optional but helpful):**
   ```
   TF_CPP_MIN_LOG_LEVEL=2
   CUDA_VISIBLE_DEVICES=
   ```

4. **Save and Redeploy**

### **Step 2: Verify Gunicorn is Running**

After redeploy, check logs. You should see:
```
[INFO] Starting gunicorn 21.x.x
[INFO] Listening at: http://0.0.0.0:5000
```

**NOT:**
```
* Running on all addresses (0.0.0.0)
* Running on http://127.0.0.1:5000
WARNING: This is a development server...
```

### **Step 3: Monitor First Request**

First request after deployment will:
1. Download model (~580MB) - takes 30-60 seconds
2. Load model into memory - takes 10-20 seconds
3. Process request - takes 5-10 seconds
4. **Total: 45-90 seconds** (one-time only)

Subsequent requests should be **5-10 seconds** (model already loaded).

### **Step 4: Test Health Endpoint**

```bash
curl https://your-service.onrender.com/health
```

Should return quickly (model already loaded).

---

## üìä Performance Expectations

### **Free/Starter Tier (512MB RAM):**
- ‚ùå **Not recommended** - Memory too tight
- First request: 60-90 seconds (model download + load)
- Subsequent requests: 10-20 seconds (if service stays running)
- Service sleeps after 15 min ‚Üí Model cache lost ‚Üí Re-download needed

### **Standard Tier (2GB RAM):**
- ‚úÖ **Recommended** - Comfortable memory
- First request: 45-60 seconds (model download + load)
- Subsequent requests: 5-10 seconds
- Service stays running ‚Üí Model stays cached ‚Üí Fast responses

---

## üîç Troubleshooting

### **Issue: Still seeing "python app.py" in logs**

**Solution:**
1. Render Dashboard ‚Üí Service ‚Üí Settings
2. Find "Start Command" field
3. **Delete the value** (make it empty)
4. Save and redeploy
5. Render will use Procfile automatically

### **Issue: Model keeps re-downloading**

**Causes:**
- Service restarting frequently (memory issues)
- Cache directory being cleared
- Service sleeping (free tier)

**Solutions:**
- Upgrade to Standard plan (stays running)
- Check memory usage in Render dashboard
- Model should cache in `/opt/render/.deepface/weights/`

### **Issue: Memory errors**

**Solution:**
- Upgrade to Standard plan (2GB RAM)
- Or reduce model size (use lighter model - not recommended, less accurate)

---

## ‚úÖ Summary of Fixes Applied

1. ‚úÖ **Suppressed TensorFlow/CUDA warnings** - Cleaner logs
2. ‚úÖ **Pre-load model at startup** - Faster first request, model cached
3. ‚úÖ **Force CPU mode** - No GPU errors
4. ‚úÖ **Better error handling** - More informative messages
5. ‚úÖ **Procfile uses gunicorn** - Production-ready server

## üéØ Next Steps

1. **Update Render Start Command** (remove explicit command)
2. **Upgrade to Standard plan** (2GB RAM) - Highly recommended
3. **Redeploy service**
4. **Monitor logs** - Should see gunicorn, not Flask dev server
5. **Test health endpoint** - Should respond quickly after first request

---

## üí° Pro Tips

1. **Warm-up request:** After deployment, make a test request to trigger model download
2. **Monitor memory:** Check Render dashboard for memory usage
3. **Keep-alive:** Set up a cron job to ping `/health` every 10 minutes (prevents sleep)
4. **Logs:** Check Render logs for any errors or warnings

